


import pandas as pd

# Load Dataset_A.csv into a DataFrame
df_A = pd.read_csv('Dataset_A.csv')

# Load Dataset_B.csv into a DataFrame
df_B = pd.read_csv('Dataset_B.csv')

# Display the first few rows of both DataFrames
df_A.head(), df_B.head()





# Renaming the columns to align similar data for merging
df_B_renamed = df_B.rename(columns={ "PatientID": "SampleID"})  # Assuming "PatientID" and "SampleID" are the same

# Replacing 'P' with 'S' in SampleID column of Dataset_B to match the format of SampleID in Dataset_A
df_B_renamed['SampleID'] = df_B_renamed['SampleID'].str.replace('P', 'S')

# Merging the datasets on the common column "SampleID"
merged_df = pd.merge(df_A, df_B_renamed, on="SampleID", how="outer")


merged_df.head()


# Checking for duplicate rows and removing them
merged_df_no_duplicates = merged_df.drop_duplicates()

# Identifying missing data
missing_data = merged_df_no_duplicates.isnull().sum()

print(missing_data)





# Get all rows that contain NaN values
rows_with_nan = merged_df_no_duplicates[merged_df_no_duplicates.isnull().any(axis=1)]

# Print the rows with NaN values
print(rows_with_nan)





merged_df_filled = merged_df_no_duplicates.fillna("Unknown")





import numpy as np

# Dropping rows with 'Unknown' values
df_cleaned = merged_df_filled.replace("Unknown", pd.NA).dropna()

# Checking for anomalies in columns
categorical_columns = ['Mutation', 'Alteration', 'Gene', 'Impact' ,'Effect', 'Chromosome']

# Getting the unique values in each column to inspect for anomalies
anomalies_categorical = {col: np.array(sorted(df_cleaned[col].unique())) for col in categorical_columns}

# Checking the range of the 'Location' column for outliers (assuming valid locations are non-negative)
location_outliers = df_cleaned[df_cleaned['Location'] < 0]

# Displaying results: unique values in categorical columns and any outliers in 'Location'
anomalies_categorical, location_outliers








merged_data = merged_df_filled
merged_data








import os
import matplotlib.pyplot as plt
from collections import Counter
import spacy

# Load the pre-trained model for NER (spacy's model for English)
nlp = spacy.load('en_core_web_sm')

# List of article file paths
file_paths = [
    'article_1.txt',
    'article_2.txt',
    'article_3.txt',
    'article_4.txt',
    'article_5.txt'
]

# Initialize a Counter to hold entity frequencies
entity_counter = Counter()

# Function to extract named entities from text
def extract_entities(text):
    doc = nlp(text)
    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'GPE', 'PERSON', 'NORP', 'FAC', 'PRODUCT']] # ENTITY TYPES
    return entities

# Process each article and count the entities
for file_path in file_paths:
    with open(file_path, 'r') as file:
        text = file.read()
        entities = extract_entities(text)
        entity_counter.update(entities)

# Get the most common entities
most_common_entities = entity_counter.most_common(10)

# Separate the entities and their counts for plotting
entities, counts = zip(*most_common_entities)

# Plot the most common entities
plt.figure(figsize=(10, 6))
plt.barh(entities, counts, color='skyblue')
plt.xlabel('Frequency')
plt.ylabel('Biological Entities')
plt.title('Most Common Biological Entities Across Articles')
plt.gca().invert_yaxis()
plt.tight_layout()

# Show the plot
plt.show()




